UHG

Explain about the current application, How did you developed cicd pipeline for..?
- Ours is the backend is a dot net application front end is a node js application 
- Our jenkins pipleine has build agent
- it ha svarious stages and steps such as build test deploy, we have post build actions sucess and failure, her ethe stages and steps consist of different repositories
- we have the docker hub registeris, aws ecr registeries and we have sonar qube and the nexus artifactory
- Nexus artifactory and aws ecr for pre prod
- for production we have Docker hub and Nexus artifactory
- Push and pull of the images happen through CICD pipeline and apart from the Sonar Qube and App scan is integrated with that 
- Kubernetes cluster details which is installed through helm chart and helm upgrade 
- Helm install is for resource deployment
- Helm upgrade is for there is an existing resources, there is an existing resource to upgrade that we use helm upgrade
- Apart from that security stand point will be using the trivy, clay
- Sonar Qube is checked to maintain the code quality the code is compiled and checked if 80 % code quality  check is been done
- Jenkins is tagged to git and git hub versioning its been done through web hooks and here we follow the multi branch pipeline dev prod pre prod hot fix and releas branch 

- When a deveoper pushes the code from feature branch, yu guys are mainiating if the code quality is less than 80 percent will  it merge into the develop branch code.
- Individual branches itself has sonar code quality check and if its going less than 80 percent there are alerts which is triggered and an notification email is sent to the
  team and so the code quality check is going less than 80 percent it wont be merged, if its satisfying the pre condition of 80 percent then only it goes to the PR apporval
  and to the main dev leads.
- If its having 80 percent quality check then only its getting need to the dev branch.
- Until the review is approved only it wont merge.

- Your deploying to kubernetes cluster, from your jenkins how your kubernetes cluster authentication is happening..?
-  So here the EKS cluster namespace , we ar Enot using the default namespace, here we are mentinoing the namespace and also thee cluster details and in our case lon cluster details
   the cluter ip the AWS EKS  cluster along with helm command along with the namespace.
- All of these are mentioned application gets deployed.

- Dev, Pre prOd, application is deploying on pre prod is it happening simultaenously or how is it happenineng, is it happening paralley;..?
- Parallely how are you authenticating different namespaces..?
- The namespace here is the same
- How differnet its
- You mentinong the IP address environment, namespace is same.

- For production ours is on prem server, we are using manually..?
- we are using ISO images, whwnver we are pushing the images to the production team and the management team is notified.Its a rocky linux machine
- The setups is the same.
Because of the customer 

- How many total namespaces
4 namespaces
1 - Namespace of front end and the back end 
1 - Default
1 - Istio (Routing Traffic of ISTIO)
1 - Data Related stuff , Reddis Rabbit MQ stack

- Source Code Repository on prem 
- Source code is maintaining on prem - git github, jenkins is on prem

- Build agents
- how are they mainatined
- Build agents are specific for tech stack such as nodejs, dot net 
- We remove spaces on regular basis.
- Manually go and clean those images
- Sometimes the docker images are more than required.
- CICD pipeline which is cleaning of spaces happening on regular basis

- Where your nodes are running
- Nodes are on single machine or multipe machines
- Our Jenkins Main node single 
- Build agents they are setup in such an way 
- Jenkins 

- As your maintaing as nodes 
- Docker agents on Kubernetes 
- Whenver a job triggers docker agent the job needs to run on the docker container
- Jenkins file also configuring the docker agent, from there we can trigger the all the images can be used.
- 
AWS ECR is dev stagong
Docker Hub is for production 

- lets say we are deploying application in 1.1
- Due to some reasons
- Due to application issues
- How your helm staragies work 
- Will your existing verison will be there or not.
- I will helm uninstall any specific namespaces.
- Clear any images
- Manually clearly images
- I will deploy the application.
- copyy the images 
- define spcieicf namespace spaces
- helm upgrade name of the resource and namepsace

- How:
- Helm Roll back stragies

- helm rollback <release-name> [REVISION]

- Example:
- helm rollback my-app 2

- Check History:
- helm history my-app

- We can also blue green deployment
- we can go and update config maps for deployment.
- Your pod is been deploying but  pod is not coming up , how do you troubelshoot,

✅ Step-by-Step Pod Troubleshooting
1. Check Pod Status
- kubectl get pod <pod-name> -n <namespace>
- Look for status: CrashLoopBackOff, Pending, ImagePullBackOff, etc.

2. Describe the Pod
- kubectl describe pod <pod-name> -n <namespace>
- Check:
- Events (at bottom)
- Scheduling issues (node selectors, taints, resources)
- Liveness/readiness probe failures

3. Check Pod Logs
- kubectl logs <pod-name> -n <namespace>       # For single container
- kubectl logs <pod-name> -c <container-name>   # For multi-container
- Look for:
- Application errors
- Configuration issues (ports, DB, env vars)

4. Check Node Health
- kubectl get nodes
- kubectl describe node <node-name>
- Ensure:
- Node is Ready
- Sufficient CPU/memory

5. Check Events in Namespace
- kubectl get events -n <namespace> --sort-by=.metadata.creationTimestamp
- Find:
- Scheduling errors
- Image pull issues
- Network problems

6. Check for Image Pull Issues
- kubectl describe pod <pod-name>
Look for:
- ImagePullBackOff
- Wrong image name/tag
- Missing imagePullSecrets

7. Check Resource Limits
- Pod may be evicted or pending due to CPU/Memory limits.
- Use describe to inspect.

8. Check Network/DNS Issues
- kubectl exec -it <pod> -- nslookup <service-name>
- Ensure:
- DNS is resolving
- Network policies aren’t blocking traffic

9. Check PVC or Volume Mount Issues
kubectl describe pvc <pvc-name> -n <namespace>
Look for:
Pending or Failed status
StorageClass or access mode mismatches

10. Check Init Containers
kubectl describe pod <pod-name>
Ensure init containers completed successfully.

